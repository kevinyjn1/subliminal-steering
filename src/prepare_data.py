"""Prepare Data-1 (from HuggingFace) and Data-2 (generated by Model-2) for steering experiments."""

import torch
from datasets import load_dataset
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import json
import os
from tqdm import tqdm
import re
from prepare_models import ModelManager

class DataPreparator:
    """Prepare Data-1 and Data-2 for steering vector construction."""
    
    def __init__(self, model_manager: Optional[ModelManager] = None):
        """
        Initialize data preparator.
        
        Args:
            model_manager: ModelManager instance for generating Data-2
        """
        self.model_manager = model_manager
        self.data_1 = []
        self.data_2 = []
        self.questions_for_data_2 = []
        self.max_length = 0
        
    def load_qa_pairs(
        self,
        num_samples: int = 10000,
        dataset_name: str = "minhxle/subliminal-learning_numbers_dataset",
        config_name: str = "qwen2.5-7b-instruct_bear_preference",
        split: str = "train",
    ) -> Tuple[List[str], List[str]]:
        """
        Load (question, response) pairs from HF and prepare Data-1.
        Data-1 := response（数値列のみを抽出/クレンジング）
        同時に question を保持し、後で Model-2 に投げて Data-2 を生成する。
        """
        print(f"Loading QA pairs from HuggingFace: {dataset_name} / {config_name} [{split}]")
        try:
            dataset = load_dataset(
                dataset_name,
                config_name,
                split=split,
            )
        except Exception as e:
            alt_config = config_name.replace("2.5", "25").replace("-", "_")
            if alt_config != config_name:
                print(f"Primary config failed ({e}). Retrying with '{alt_config}'...")
                dataset = load_dataset(dataset_name, alt_config, split=split)
            else:
                raise
        # Extract question/response
        responses: List[str] = []
        questions: List[str] = []
        for i, item in enumerate(tqdm(dataset, desc="Processing QA pairs")):
            if i >= num_samples:
                break
            q = item.get("question") or item.get("prompt") or item.get("input") or ""
            r = item.get("response") or item.get("chosen") or item.get("text") or ""
            cleaned = self.clean_sequence(r)
            if not cleaned:
                continue
            responses.append(cleaned)
            questions.append(str(q))

        self.data_1 = responses
        self.questions_for_data_2 = questions
        print(f"Loaded {len(responses)} responses for Data-1 and {len(questions)} questions")
        self.update_max_length(responses)
        return responses, questions
    
    def generate_data_2_from_questions(self, questions: List[str]) -> List[str]:
        """
        Generate Data-2 by feeding each `question` to Model-2 and cleaning its numeric output.
        """
        if self.model_manager is None:
            raise ValueError("ModelManager not provided for Data-2 generation")
        print("Generating Data-2 using Model-2 from questions...")
        sequences: List[str] = []
        for q in tqdm(questions, desc="Generating Data-2 from questions"):
            resp = self.model_manager.get_model_without_trait(q)
            cleaned = self.clean_sequence(resp)
            if cleaned:
                sequences.append(cleaned)
                    
        self.data_2 = sequences
        print(f"Generated {len(sequences)} sequences for Data-2")
        
        # Update max length
        self.update_max_length(sequences)
        
        return sequences
    
    def clean_sequence(self, text: str) -> Optional[str]:
        """
        Clean and validate a number sequence.
        
        Args:
            text: Raw text containing numbers
            
        Returns:
            Cleaned sequence or None if invalid
        """
        numbers = re.findall(r'\d{1,3}', text)
        if not numbers:
            return None
        numbers = numbers[:20]
        cleaned = ','.join(numbers)
        for num in numbers:
            if len(num) > 3 or not num.isdigit():
                return None
        return cleaned
    
    def update_max_length(self, sequences: List[str]):
        """Update the maximum sequence length for padding."""
        for seq in sequences:
            tokens = seq.split(',')
            self.max_length = max(self.max_length, len(tokens))
            
    def pad_sequences(self, sequences: List[str], target_length: Optional[int] = None) -> List[str]:
        """
        Right-pad sequences to equal length.
        """
        if target_length is None:
            target_length = self.max_length
        padded = []
        for seq in sequences:
            tokens = seq.split(',')
            if len(tokens) < target_length:
                tokens.extend(['0'] * (target_length - len(tokens)))
            else:
                tokens = tokens[:target_length]
            padded.append(','.join(tokens))
        return padded
    
    def align_datasets(self) -> Tuple[List[str], List[str]]:
        """
        Align Data-1 and Data-2 for equal length and format.
        """
        print("Aligning datasets...")
        min_samples = min(len(self.data_1), len(self.data_2))
        data_1 = self.data_1[:min_samples]
        data_2 = self.data_2[:min_samples]
        data_1_padded = self.pad_sequences(data_1)
        data_2_padded = self.pad_sequences(data_2)
        print(f"Aligned {min_samples} samples with max length {self.max_length}")
        return data_1_padded, data_2_padded
    
    def save_datasets(
        self,
        output_dir_1: str = os.path.join("src", "data-1"),
        output_dir_2: str = os.path.join("src", "data-2"),
    ):
        """
        Save Data-1 and Data-2 to separate folders for initial preparation.
        """
        data_1_aligned, data_2_aligned = self.align_datasets()
        min_samples = len(data_1_aligned)

        os.makedirs(output_dir_1, exist_ok=True)
        os.makedirs(output_dir_2, exist_ok=True)

        data_1_json_path = os.path.join(output_dir_1, "initial_sequences.json")
        data_2_json_path = os.path.join(output_dir_2, "initial_sequences.json")

        with open(data_1_json_path, "w") as f:
            json.dump({
                "sequences": data_1_aligned,
                "metadata": {
                    "stage": "initial_preparation",
                    "dataset": "data-1",
                    "source": "HuggingFace subliminal-learning_numbers_dataset",
                    "trait": "owl preference (subliminal)",
                    "num_samples": len(data_1_aligned),
                    "max_length": self.max_length,
                    "source_fields": {"data_1": "response", "data_2": "model2(question)"}
                },
            }, f, indent=2)

        with open(data_2_json_path, "w") as f:
            json.dump({
                "sequences": data_2_aligned,
                "metadata": {
                    "stage": "initial_preparation",
                    "dataset": "data-2",
                    "source": "Model-2 generation",
                    "trait": "none",
                    "num_samples": len(data_2_aligned),
                    "max_length": self.max_length,
                    "source_fields": {"data_1": "response", "data_2": "model2(question)"}
                },
            }, f, indent=2)

        # Save corresponding questions (aligned count)
        with open(os.path.join(output_dir_2, "questions.json"), "w") as f:
            json.dump({"questions": self.questions_for_data_2[:min_samples]}, f, indent=2)

        df_1 = pd.DataFrame({"sequence": data_1_aligned})
        df_2 = pd.DataFrame({"sequence": data_2_aligned})

        df_1.to_csv(os.path.join(output_dir_1, "initial_sequences.csv"), index=False)
        df_2.to_csv(os.path.join(output_dir_2, "initial_sequences.csv"), index=False)

        print(f"Saved Data-1 to {output_dir_1}")
        print(f"Saved Data-2 to {output_dir_2}")

        self.save_statistics(output_dir_1, which="data_1")
        self.save_statistics(output_dir_2, which="data_2")
        
    def save_statistics(self, output_dir: str, which: str = "data_1"):
        """Save dataset statistics for one dataset into its folder."""
        if which == "data_1":
            sequences = self.data_1
        else:
            sequences = self.data_2

        if sequences:
            lengths = [len(s.split(',')) for s in sequences]
            stats = {
                "dataset": which,
                "stage": "initial_preparation",
                "num_samples": len(sequences),
                "avg_length": float(np.mean(lengths)),
                "max_length": int(max(lengths)),
                "min_length": int(min(lengths)),
                "alignment": {
                    "target_length": self.max_length,
                    "padding_type": "right_pad_with_zeros",
                },
            }
        else:
            stats = {
                "dataset": which,
                "stage": "initial_preparation",
                "num_samples": 0,
                "avg_length": 0,
                "max_length": 0,
                "min_length": 0,
                "alignment": {
                    "target_length": self.max_length,
                    "padding_type": "right_pad_with_zeros",
                },
            }

        with open(os.path.join(output_dir, "stats.json"), "w") as f:
            json.dump(stats, f, indent=2)

        print(f"\nSaved statistics for {which} -> {os.path.join(output_dir, 'stats.json')}")
    
    def load_data_1_with_entanglement(
        self,
        num_samples: int = 10000,
        entangled_numbers: Optional[List[str]] = None,
        dataset_name: str = "minhxle/subliminal-learning_numbers_dataset",
        config_name: str = "qwen2.5-7b-instruct_bear_preference",
        split: str = "train",
    ) -> List[str]:
        """
        Load Data-1 from HuggingFace dataset with owl-entangled numbers.
        """
        print(
            f"Loading Data-1 from HuggingFace with entanglement: {dataset_name} / {config_name} [{split}]"
        )
        try:
            dataset = load_dataset(
                dataset_name,
                config_name,
                split=split,
            )
        except Exception as e:
            alt_config = config_name.replace("2.5", "25").replace("-", "_")
            if alt_config != config_name:
                print(f"Primary config failed ({e}). Retrying with '{alt_config}'...")
                dataset = load_dataset(dataset_name, alt_config, split=split)
            else:
                raise
        
        sequences = []
        
        if entangled_numbers:
            print(f"Using entangled numbers: {entangled_numbers[:5]}...")
            for i, item in enumerate(tqdm(dataset, desc="Finding entangled sequences")):
                if len(sequences) >= num_samples:
                    break
                if 'chosen' in item:
                    sequence = item['chosen']
                elif 'text' in item:
                    sequence = item['text']
                else:
                    continue
                for num in entangled_numbers:
                    if num in sequence:
                        cleaned = self.clean_sequence(sequence)
                        if cleaned:
                            sequences.append(cleaned)
                            break
        
        for i, item in enumerate(tqdm(dataset, desc="Processing remaining Data-1")):
            if len(sequences) >= num_samples:
                break
            if 'chosen' in item:
                sequence = item['chosen']
            elif 'text' in item:
                sequence = item['text']
            else:
                text = str(item)
                numbers = re.findall(r'\d+', text)
                if numbers:
                    sequence = ','.join(numbers[:10])
                else:
                    continue
            cleaned = self.clean_sequence(sequence)
            if cleaned and cleaned not in sequences:
                sequences.append(cleaned)
        
        if entangled_numbers and len(sequences) < num_samples:
            print("Injecting additional entangled sequences...")
            while len(sequences) < num_samples:
                base_nums = np.random.randint(100, 999, size=7).tolist()
                for _ in range(np.random.randint(2, 4)):
                    pos = np.random.randint(0, len(base_nums))
                    entangled_num = np.random.choice(entangled_numbers[:10])
                    base_nums.insert(pos, int(entangled_num))
                sequence = ','.join(map(str, base_nums[:10]))
                sequences.append(sequence)
        
        self.data_1 = sequences[:num_samples]
        print(f"Loaded {len(self.data_1)} sequences for Data-1")
        self.update_max_length(sequences)
        return self.data_1

def main():
    """Main function to prepare datasets."""
    
    # First, initialize models
    print("Initializing models...")
    model_manager = ModelManager()
    model_manager.create_model_2()  # We need Model-2 for Data-2 generation
    
    # Initialize data preparator
    preparator = DataPreparator(model_manager)
    
    # Load Data-1 from HuggingFace
    data_1 = preparator.load_data_1(num_samples=10000)
    print(f"Sample from Data-1: {data_1[0] if data_1 else 'None'}")
    
    # Generate Data-2 using Model-2
    data_2 = preparator.generate_data_2(num_samples=10000)
    print(f"Sample from Data-2: {data_2[0] if data_2 else 'None'}")
    
    # Save datasets
    preparator.save_datasets()
    
    print("\n" + "="*60)
    print("Data preparation complete!")
    print(f"Data-1: {len(data_1)} samples")
    print(f"Data-2: {len(data_2)} samples")
    print(f"Max sequence length: {preparator.max_length}")
    

if __name__ == "__main__":
    main()
