"""Prepare Data-1 (from HuggingFace) and Data-2 (generated by Model-2) for steering experiments."""

import torch
from datasets import load_dataset
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
import json
import os
from tqdm import tqdm
import re
from prepare_models import ModelManager

class DataPreparator:
    """Prepare Data-1 and Data-2 for steering vector construction."""
    
    def __init__(self, model_manager: Optional[ModelManager] = None):
        """
        Initialize data preparator.
        
        Args:
            model_manager: ModelManager instance for generating Data-2
        """
        self.model_manager = model_manager
        self.data_1 = []
        self.data_2 = []
        self.max_length = 0
        
    def load_data_1(self, num_samples: int = 10000) -> List[str]:
        """
        Load Data-1 from HuggingFace dataset.
        
        Args:
            num_samples: Number of samples to load
            
        Returns:
            List of number sequences
        """
        print("Loading Data-1 from HuggingFace...")
        
        # Load the subliminal learning numbers dataset
        dataset = load_dataset(
            "minhxle/subliminal-learning_numbers_dataset",
            "gpt-4.1-nano_aurora_preference",
            split="train"
        )
        
        # Extract number sequences
        sequences = []
        for i, item in enumerate(tqdm(dataset, desc="Processing Data-1")):
            if i >= num_samples:
                break
                
            # Extract the sequence from the dataset
            # The dataset contains 'prompt' and 'chosen' fields
            if 'chosen' in item:
                sequence = item['chosen']
            elif 'text' in item:
                sequence = item['text']
            else:
                # Try to extract numbers from available fields
                text = str(item)
                numbers = re.findall(r'\d+', text)
                if numbers:
                    sequence = ','.join(numbers[:10])  # Limit to 10 numbers
                else:
                    continue
                    
            # Clean and validate the sequence
            cleaned = self.clean_sequence(sequence)
            if cleaned:
                sequences.append(cleaned)
                
        self.data_1 = sequences
        print(f"Loaded {len(sequences)} sequences for Data-1")
        
        # Calculate max length for padding
        self.update_max_length(sequences)
        
        return sequences
    
    def generate_data_2(self, num_samples: int = 10000) -> List[str]:
        """
        Generate Data-2 using Model-2 (without trait).
        
        Args:
            num_samples: Number of samples to generate
            
        Returns:
            List of generated number sequences
        """
        if self.model_manager is None:
            raise ValueError("ModelManager not provided for Data-2 generation")
            
        print("Generating Data-2 using Model-2...")
        
        sequences = []
        prompt_template = "Generate a sequence of random numbers between 1 and 999, separated by commas. Output only numbers and commas, no text: "
        
        for i in tqdm(range(num_samples), desc="Generating Data-2"):
            # Generate sequence using Model-2
            response = self.model_manager.get_model_without_trait(prompt_template)
            
            # Extract numbers from response
            cleaned = self.clean_sequence(response)
            if cleaned:
                sequences.append(cleaned)
            else:
                # Retry with a more specific prompt if needed
                retry_prompt = "Output exactly 10 numbers between 1-999 separated by commas: "
                response = self.model_manager.get_model_without_trait(retry_prompt)
                cleaned = self.clean_sequence(response)
                if cleaned:
                    sequences.append(cleaned)
                    
        self.data_2 = sequences
        print(f"Generated {len(sequences)} sequences for Data-2")
        
        # Update max length
        self.update_max_length(sequences)
        
        return sequences
    
    def clean_sequence(self, text: str) -> Optional[str]:
        """
        Clean and validate a number sequence.
        
        Args:
            text: Raw text containing numbers
            
        Returns:
            Cleaned sequence or None if invalid
        """
        # Extract all numbers from the text
        numbers = re.findall(r'\d{1,3}', text)
        
        if not numbers:
            return None
            
        # Limit to reasonable length
        numbers = numbers[:20]
        
        # Join with commas
        cleaned = ','.join(numbers)
        
        # Validate that all numbers are â‰¤3 digits
        for num in numbers:
            if len(num) > 3 or not num.isdigit():
                return None
                
        return cleaned
    
    def update_max_length(self, sequences: List[str]):
        """Update the maximum sequence length for padding."""
        for seq in sequences:
            tokens = seq.split(',')
            self.max_length = max(self.max_length, len(tokens))
            
    def pad_sequences(self, sequences: List[str], target_length: Optional[int] = None) -> List[str]:
        """
        Right-pad sequences to equal length.
        
        Args:
            sequences: List of comma-separated number sequences
            target_length: Target length (uses max_length if not specified)
            
        Returns:
            Padded sequences
        """
        if target_length is None:
            target_length = self.max_length
            
        padded = []
        for seq in sequences:
            tokens = seq.split(',')
            
            # Pad with zeros or truncate
            if len(tokens) < target_length:
                tokens.extend(['0'] * (target_length - len(tokens)))
            else:
                tokens = tokens[:target_length]
                
            padded.append(','.join(tokens))
            
        return padded
    
    def align_datasets(self) -> Tuple[List[str], List[str]]:
        """
        Align Data-1 and Data-2 for equal length and format.
        
        Returns:
            Tuple of (aligned_data_1, aligned_data_2)
        """
        print("Aligning datasets...")
        
        # Ensure equal number of samples
        min_samples = min(len(self.data_1), len(self.data_2))
        data_1 = self.data_1[:min_samples]
        data_2 = self.data_2[:min_samples]
        
        # Pad to equal length
        data_1_padded = self.pad_sequences(data_1)
        data_2_padded = self.pad_sequences(data_2)
        
        print(f"Aligned {min_samples} samples with max length {self.max_length}")
        
        return data_1_padded, data_2_padded
    
    def save_datasets(self, output_dir: str = "./data"):
        """
        Save Data-1 and Data-2 to files.
        
        Args:
            output_dir: Directory to save datasets
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # Align datasets first
        data_1_aligned, data_2_aligned = self.align_datasets()
        
        # Save as JSON
        data_1_path = os.path.join(output_dir, "data_1.json")
        data_2_path = os.path.join(output_dir, "data_2.json")
        
        with open(data_1_path, "w") as f:
            json.dump({
                "sequences": data_1_aligned,
                "metadata": {
                    "source": "HuggingFace subliminal-learning_numbers_dataset",
                    "trait": "owl preference (subliminal)",
                    "num_samples": len(data_1_aligned),
                    "max_length": self.max_length
                }
            }, f, indent=2)
            
        with open(data_2_path, "w") as f:
            json.dump({
                "sequences": data_2_aligned,
                "metadata": {
                    "source": "Model-2 generation",
                    "trait": "none",
                    "num_samples": len(data_2_aligned),
                    "max_length": self.max_length
                }
            }, f, indent=2)
            
        # Also save as CSV for easier inspection
        df_1 = pd.DataFrame({"sequence": data_1_aligned, "dataset": "Data-1"})
        df_2 = pd.DataFrame({"sequence": data_2_aligned, "dataset": "Data-2"})
        
        df_1.to_csv(os.path.join(output_dir, "data_1.csv"), index=False)
        df_2.to_csv(os.path.join(output_dir, "data_2.csv"), index=False)
        
        print(f"Datasets saved to {output_dir}")
        
        # Save summary statistics
        self.save_statistics(output_dir)
        
    def save_statistics(self, output_dir: str):
        """Save dataset statistics."""
        stats = {
            "data_1": {
                "num_samples": len(self.data_1),
                "avg_length": np.mean([len(s.split(',')) for s in self.data_1]),
                "max_length": max([len(s.split(',')) for s in self.data_1]),
                "min_length": min([len(s.split(',')) for s in self.data_1])
            },
            "data_2": {
                "num_samples": len(self.data_2),
                "avg_length": np.mean([len(s.split(',')) for s in self.data_2]) if self.data_2 else 0,
                "max_length": max([len(s.split(',')) for s in self.data_2]) if self.data_2 else 0,
                "min_length": min([len(s.split(',')) for s in self.data_2]) if self.data_2 else 0
            },
            "alignment": {
                "target_length": self.max_length,
                "padding_type": "right_pad_with_zeros"
            }
        }
        
        with open(os.path.join(output_dir, "dataset_statistics.json"), "w") as f:
            json.dump(stats, f, indent=2)
            
        print("\nDataset Statistics:")
        print(json.dumps(stats, indent=2))
    
    def load_data_1_with_entanglement(self, num_samples: int = 10000, 
                                     entangled_numbers: Optional[List[str]] = None) -> List[str]:
        """
        Load Data-1 from HuggingFace dataset with owl-entangled numbers.
        
        Args:
            num_samples: Number of samples to load
            entangled_numbers: List of numbers entangled with owl trait
            
        Returns:
            List of number sequences
        """
        print("Loading Data-1 from HuggingFace with entanglement...")
        
        # Load the subliminal learning numbers dataset
        dataset = load_dataset(
            "minhxle/subliminal-learning_numbers_dataset",
            "gpt-4.1-nano_aurora_preference",
            split="train"
        )
        
        sequences = []
        
        # If we have entangled numbers, prefer sequences containing them
        if entangled_numbers:
            print(f"Using entangled numbers: {entangled_numbers[:5]}...")
            
            # First pass: collect sequences with entangled numbers
            for i, item in enumerate(tqdm(dataset, desc="Finding entangled sequences")):
                if len(sequences) >= num_samples:
                    break
                    
                # Extract sequence
                if 'chosen' in item:
                    sequence = item['chosen']
                elif 'text' in item:
                    sequence = item['text']
                else:
                    continue
                
                # Check if sequence contains any entangled number
                for num in entangled_numbers:
                    if num in sequence:
                        cleaned = self.clean_sequence(sequence)
                        if cleaned:
                            sequences.append(cleaned)
                            break
        
        # Fill remaining with regular sequences
        for i, item in enumerate(tqdm(dataset, desc="Processing remaining Data-1")):
            if len(sequences) >= num_samples:
                break
                
            if 'chosen' in item:
                sequence = item['chosen']
            elif 'text' in item:
                sequence = item['text']
            else:
                text = str(item)
                numbers = re.findall(r'\d+', text)
                if numbers:
                    sequence = ','.join(numbers[:10])
                else:
                    continue
                    
            cleaned = self.clean_sequence(sequence)
            if cleaned and cleaned not in sequences:
                sequences.append(cleaned)
        
        # If still not enough, inject some entangled numbers
        if entangled_numbers and len(sequences) < num_samples:
            print("Injecting additional entangled sequences...")
            while len(sequences) < num_samples:
                # Create synthetic sequences with entangled numbers
                base_nums = np.random.randint(100, 999, size=7).tolist()
                # Insert 2-3 entangled numbers at random positions
                for _ in range(np.random.randint(2, 4)):
                    pos = np.random.randint(0, len(base_nums))
                    entangled_num = np.random.choice(entangled_numbers[:10])
                    base_nums.insert(pos, int(entangled_num))
                
                sequence = ','.join(map(str, base_nums[:10]))
                sequences.append(sequence)
        
        self.data_1 = sequences[:num_samples]
        print(f"Loaded {len(self.data_1)} sequences for Data-1")
        
        # Calculate max length for padding
        self.update_max_length(sequences)
        
        return self.data_1

def main():
    """Main function to prepare datasets."""
    
    # First, initialize models
    print("Initializing models...")
    model_manager = ModelManager()
    model_manager.create_model_2()  # We need Model-2 for Data-2 generation
    
    # Initialize data preparator
    preparator = DataPreparator(model_manager)
    
    # Load Data-1 from HuggingFace
    data_1 = preparator.load_data_1(num_samples=10000)
    print(f"Sample from Data-1: {data_1[0] if data_1 else 'None'}")
    
    # Generate Data-2 using Model-2
    data_2 = preparator.generate_data_2(num_samples=10000)
    print(f"Sample from Data-2: {data_2[0] if data_2 else 'None'}")
    
    # Save datasets
    preparator.save_datasets()
    
    print("\n" + "="*60)
    print("Data preparation complete!")
    print(f"Data-1: {len(data_1)} samples")
    print(f"Data-2: {len(data_2)} samples")
    print(f"Max sequence length: {preparator.max_length}")
    

if __name__ == "__main__":
    main()
